{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Allow duplicates\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "\n",
    "PATH = \".\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join (PATH,\"images\", fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is supposed to demo the Vanishing/Exploding Gradients (under or over firing)\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Test out He initialization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable hidden1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-5885a7e769e1>\", line 5, in <module>\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n  File \"/Users/udoucal/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/udoucal/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7200c47773a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhe_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n\u001b[0;32m----> 5\u001b[0;31m                           kernel_initializer=he_init, name=\"hidden1\")\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 188\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \"\"\"\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[0;31m# Only call `build` if the user has manually overridden the build method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             getter=vs.get_variable)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    605\u001b[0m     new_variable = getter(\n\u001b[1;32m    606\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable hidden1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-5885a7e769e1>\", line 5, in <module>\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n  File \"/Users/udoucal/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/udoucal/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "## tf_layers.dense normally uses Xavier initialization but you can override that to use He.\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note we have no idea how useful that would be....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try out \"leaky RelU\" which allows for some but not as \"much\" for values below 0.\n",
    "## Hyperparameter \"alpha\" controls the level of leak (default of .01, but some research thinks .02 is better)\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leaky ReLU in Tensor Flow\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-5885a7e769e1>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/udoucal/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "## Note activation parameter\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a NN on MNIST using ReLU (2 inner layers)\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scope into \"dnn\"\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Name the \"loss\"\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify Learning Rate and Optimizer of Gradient Descent\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have a model defined we can load the data  (note loading from keras not tf examples)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to batch load the data with parameters of epochs and batch size\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9494\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9658\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.971\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9762\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9774\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9778\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "## Run the data in the graph\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now try ELU\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualize ELU\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the whole thing with ELU\n",
    "\n",
    "## Scope into \"dnn2\"\n",
    "with tf.name_scope(\"dnn2\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to execute operation: the operation's graph is different from the session's graph. Pass an explicit session to run(session=sess).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-88a981dbeb14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshuffle_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m     \"\"\"\n\u001b[0;32m-> 2450\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5205\u001b[0m                        \"`run(session=sess)`\")\n\u001b[1;32m   5206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5207\u001b[0;31m       raise ValueError(\"Cannot use the default session to execute operation: \"\n\u001b[0m\u001b[1;32m   5208\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5209\u001b[0m                        \u001b[0;34m\"session's graph. Pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to execute operation: the operation's graph is different from the session's graph. Pass an explicit session to run(session=sess)."
     ]
    }
   ],
   "source": [
    "#Run again... \n",
    "## Run the data in the graph\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model2_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On to SELU\n",
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note:  we found this did not work so we used \"activation=tf.nn.selu\"  in the actual layers\n",
    "\n",
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "     return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is only needed for above\n",
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test SELU on random dataset - using created not tf selu\n",
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a graph with SELU\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "## Test the whole thing with ELU\n",
    "\n",
    "## Scope into \"dnn2\"\n",
    "with tf.name_scope(\"dnn2\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.selu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.selu, name=\"hidden3\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Normalization\n",
    "##Also note that in order to run batch norm just before each hidden layer's activation function, \n",
    "## we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "## Also activation is not specified because we apply sctivation after each batch normalization\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the graph to try less repetive approach\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try using the \"partial\" function\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now build neural net with ELU and Batch Normalization\n",
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up batching\n",
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up logging\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Attempts at TF Records\n",
    "## tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DNNData = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from sklearn.preprocessing import StandardScaler\n",
    "## scaler = StandardScaler()\n",
    "## DNNDataScaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DNNDataScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Try the Widget Tool\n",
    "# from witwidget.notebook.visualization import WitConfigBuilder\n",
    "# from witwidget.notebook.visualization import WitWidget\n",
    "# config_builder = WitConfigBuilder(examples).set_estimator_and_feature_spec(classifier, feature_spec)\n",
    "# examples = df_to_examples(X_Train)\n",
    "\n",
    "\n",
    "# def df_to_examples(df, columns=None):\n",
    "#     examples = []\n",
    "#     if columns == None:\n",
    "#         columns = df.columns.values.tolist()\n",
    "#     for index, row in df.iterrows():\n",
    "#         example = tf.train.Example()\n",
    "#         for col in columns:\n",
    "#             if df[col].dtype is np.dtype(np.int64):\n",
    "#                 example.features.feature[col].int64_list.value.append(int(row[col]))\n",
    "#             elif df[col].dtype is np.dtype(np.float64):\n",
    "#                 example.features.feature[col].float_list.value.append(row[col])\n",
    "#             elif row[col] == row[col]:\n",
    "#                 example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
    "#         examples.append(example)\n",
    "#     return examples\n",
    "\n",
    "# examples = df_to_examples(X_Train)\n",
    "# WitWidget(config_builder)\n",
    "# ## Total Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient Clippsing with 5 layers\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learning_rate = 0.01\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "## threshold = .5\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the trainable variables\n",
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reusing a TensorFlow Model\n",
    "## Step #1 find the graphs structure\n",
    "reset_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all the operations you need for training (you may not know the model's structure)\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examples don't work for visualization in Juypter so used the Tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the operations by name\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are going to document your model maybe share important operations\n",
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then people can get all they need by running this command\n",
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restoring a model to a session\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the model and save a new check point \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try just using the lower layers and then add new layers\n",
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try training this new model with different layers\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Or if some reason someone has shared the Python code then you can just edit it.\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Always create a \"saver\" so not an overwrite\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                            # not shown in the book\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reusing Models from other frameworks\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is \"tedious\" as need to assign each item in the graph\n",
    "## Output is the evaluations from the hidden layer\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Or create new dedicated placeholders and \"assignment nodes\"\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ways of getting the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get collection\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Tensor by name\n",
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensuring the lower layers are \"frozen\"  - create the graph but don't use all layers in session\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference only the \"trainable\" new layers in the session\n",
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference which are for re-use and which are for new training\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## or add \"stop_gradient\" so that any layer below that will be \"frozen\"\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document parameters\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then train it\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cache the layers\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # not shown\n",
    "                                                y: y_valid})             # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start here with \"Faster Optimizers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning Rate Scheduling\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specific rates \n",
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Addressing Overfitting through regularization\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passing regularization to tf.layers.dense and playing around with partial to leverage arguments\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create scale parameter for kernel regularizer\n",
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try using the \"partial\" function\n",
    "from functools import partial\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add regularization losses to base loss\n",
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard set-up\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run it\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### or use Max_Norm_regularizer..\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training includes clipping operations with each session\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise - build a deep learning model with 5 hidden layers of 100 neurons each.  \n",
    "## Use He Initialization and ELU Activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_graph()\n",
    "\n",
    "# n_inputs = 28 * 28  # MNIST\n",
    "# n_hidden1 = 100\n",
    "# n_hidden2 = 100\n",
    "# n_hidden3 = 100\n",
    "# n_hidden4 = 100\n",
    "# n_hidden4 =100\n",
    "# n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "# y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "# with tf.name_scope(\"dnn\"):\n",
    "#     hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "#     hidden2 = tf.layers.dense(X, n_hidden2, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden2\")\n",
    "#     hidden3 = tf.layers.dense(X, n_hidden3, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden3\")\n",
    "#     hidden4 = tf.layers.dense(X, n_hidden4, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden4\")\n",
    "#     hidden5 = tf.layers.dense(X, n_hidden5, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden5\")\n",
    "#     logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"eval\"):\n",
    "#     correct = tf.nn.in_top_k(logits, y, 1)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "# learning_rate = 0.05\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Now that we have a model defined we can load the data  (note loading from keras not tf examples)\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "# X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "# y_train = y_train.astype(np.int32)\n",
    "# y_test = y_test.astype(np.int32)\n",
    "# X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "# y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shorter syntax than mine\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "n_outputs = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, \n",
    "## as we will use transfer learning for digits 5 to 9 in the next exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Digits < 5\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'he_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c8956d76155e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n\u001b[1;32m      7\u001b[0m                  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhe_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c8956d76155e>\u001b[0m in \u001b[0;36mDNNClassifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n\u001b[0;32m----> 7\u001b[0;31m                  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhe_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                  batch_norm_momentum=None, dropout_rate=None, random_state=None):\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'he_init' is not defined"
     ]
    }
   ],
   "source": [
    "## Fancy Pants with all the hyperparameters\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare to out of the box DNN Classifier\n",
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-84f1a0bea602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n\u001b[1;32m     20\u001b[0m                                 cv=3, random_state=42, verbose=2)\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrnd_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train1' is not defined"
     ]
    }
   ],
   "source": [
    "## Use randomized search for best parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                cv=3, random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "\n",
    "# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\n",
    "# fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "# rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "#                                 fit_params=fit_params, random_state=42, verbose=2)\n",
    "# rnd_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the best parameters\n",
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Did we get better?\n",
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the model\n",
    "rnd_search.best_estimator_.save(\"./my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Batch Normalization\n",
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the accuracy score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DNNClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e266918c81fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Same model with batch normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n\u001b[0m\u001b[1;32m      3\u001b[0m                            \u001b[0mn_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            batch_norm_momentum=0.95)\n\u001b[1;32m      5\u001b[0m \u001b[0mdnn_clf_bn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DNNClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "## Same model with batch normalization\n",
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42,\n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "leaky_relu() got an unexpected keyword argument 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a3d502324948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m\"activation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# you could also try exploring different numbers of hidden layers, different optimizers, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: leaky_relu() got an unexpected keyword argument 'alpha'"
     ]
    }
   ],
   "source": [
    "## Try for other parameters...\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n",
    "}\n",
    "\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50, cv=3,\n",
    "                                   random_state=42, verbose=2)\n",
    "rnd_search_bn.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "\n",
    "# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\n",
    "# fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "# rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "#                                    fit_params=fit_params, random_state=42, verbose=2)\n",
    "# rnd_search_bn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "rnd_search.best_estimator_.save(\"./my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DNNClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1e3b5b2f101e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Add Batch Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n\u001b[0m\u001b[1;32m      3\u001b[0m                         n_neurons=140, random_state=42)\n\u001b[1;32m      4\u001b[0m \u001b[0mdnn_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DNNClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "## Add Batch Normalization\n",
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "leaky_relu() missing 1 required positional argument: 'z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2b4a25a9d3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;34m\"activation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# you could also try exploring different numbers of hidden layers, different optimizers, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: leaky_relu() missing 1 required positional argument: 'z'"
     ]
    }
   ],
   "source": [
    "## Find better parameters for Batch Normalization..\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n",
    "}\n",
    "\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50, cv=3,\n",
    "                                   random_state=42, verbose=2)\n",
    "rnd_search_bn.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "\n",
    "# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\n",
    "# fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "# rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "#                                    fit_params=fit_params, random_state=42, verbose=2)\n",
    "# rnd_search_bn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnd_search_bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c225824fac77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnd_search_bn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rnd_search_bn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rnd_search_bn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
